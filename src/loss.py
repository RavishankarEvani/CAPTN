import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor
from typing import Optional
from config import CAPTNConfig


class LatentTextureAttributeLoss(nn.Module):
    """
    Computes the Latent Texture Attribute (LTA) Loss, which is a combination of:
        1. Classification Loss
        2. Orderless LTA Loss 
        3. Spatial LTA Loss

    Args:
        cfg (CAPTNConfig): Configuration object.
        alpha (float): Weight assigned to classification loss. Default is 1.0.
        beta (float): Weight assigned to orderless LTA loss. Default is 1.0.
        gamma (float): Weight assigned to spatial LTA loss. Default is 1.0.
        label_smoothing (float): Label smoothing factor for classification loss. Default is 0.0.
        epsilon (float): Small value added to softmax outputs to ensure numerical stability when computing log-probabilities. Default is 1e-10.
        
    Forward Args:
        inputs (Tensor): Logits from the model classifier, shape [B, num_classes].
        targets (Tensor): Ground truth class labels, shape [B].
        orderless_bf_combined (Tensor): Concatenated backbone-derived orderless texture representation, shape [B, C].
        orderless_elta_combined (Tensor): Concatenated learned orderless LTAs, shape [B, C].
        spatial_latent_attribute (Optional[Tensor]): Spatial Latent Attribute Representation (SLAR) generated by the model, shape [B, 3, P, P].
        extracted_patches (Optional[Tensor]): Texture patches extracted from the original image, shape [B, 3, P, P].

    Returns:
        Tensor: Final weighted LTA loss.
    """
    
    cfg: CAPTNConfig
    alpha: float
    beta: float
    gamma: float
    epsilon: float

    classification_loss: nn.CrossEntropyLoss
    
    def __init__(
        self,
        cfg: CAPTNConfig,
        alpha: float = 1.0,
        beta: float = 1.0,
        gamma: float = 1.0,
        label_smoothing: float = 0.0,
        epsilon: float = 1e-10
    ) -> None:
        super().__init__()
        
        self.cfg = cfg
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.epsilon = epsilon

        self.classification_loss = nn.CrossEntropyLoss(
            reduction='mean',
            label_smoothing=label_smoothing
        )

    def forward(
        self,
        inputs: Tensor,
        targets: Tensor,
        orderless_bf_combined: Tensor,
        orderless_elta_combined: Tensor,
        spatial_latent_attribute: Optional[Tensor] = None,
        extracted_patches: Optional[Tensor] = None
    ) -> Tensor:
        """
        Compute the total LTA loss.

        Args:
            inputs (Tensor): Logits from the model classifier, shape [B, num_classes].
            targets (Tensor): Ground truth class labels, shape [B].
            orderless_bf_combined (Tensor): Concatenated backbone-derived orderless texture representation, shape [B, C].
            orderless_elta_combined (Tensor): Concatenated learned orderless LTAs, shape [B, C].
            spatial_latent_attribute (Optional[Tensor]): Spatial Latent Attribute Representation (SLAR) generated by the model, shape [B, 3, P, P].
            extracted_patches (Optional[Tensor]): Texture patches extracted from the original image, shape [B, 3, P, P].

        Returns:
            Tensor: Final weighted LTA loss.
        """

        # Cross-Entropy Loss for classification
        loss_classification: Tensor = self.classification_loss(inputs, targets)

        # Orderless LTA Loss
        p_b: Tensor = F.softmax(
            input=orderless_bf_combined, 
            dim=1
        ) + self.epsilon
        
        p_e: Tensor = F.softmax(
            input=orderless_elta_combined, 
            dim=1
        ) + self.epsilon
        
        orderless_loss: Tensor = (p_b * (p_b.log() - p_e.log())).sum(dim=1).mean()

        # Spatial LTA Loss
        if spatial_latent_attribute is not None and extracted_patches is not None:
            spatial_loss: Tensor = F.mse_loss(
                input=spatial_latent_attribute, 
                target=extracted_patches
            )
            
            # Combine all three loss terms
            total_loss: Tensor = (
                self.alpha * loss_classification +
                self.beta * orderless_loss +
                self.gamma * spatial_loss
            )
        else:
            
            # Only classification and orderless LTA losses used
            total_loss: Tensor = (
                self.alpha * loss_classification +
                self.beta * orderless_loss
            )

        return total_loss
